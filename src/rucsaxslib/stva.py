'''
This module contains functionality intended for reading data generated by the
SVTA setup on RUCSAXS
'''
import configparser
import pandas as pd
import numpy as np
import re
from pathlib import Path
import fabio


def read_nanocalc_lrc(fn):
    model = configparser.ConfigParser()
    model.read(fn)
    return model


def get_lrc_table(fn):
    model = read_nanocalc_lrc(fn)
    thick_layer_idx = model['layers']['IndexOfThickLayer']
    model_layer_idx = model['layers']['LayersToFit'].split(';')

    row_ids = sorted(model_layer_idx, reverse=True) + [thick_layer_idx]
    rows = []
    cols = ['id', 'name', 'dmin', 'dstart', 'dmax', 'fit?']
    for row_id in row_ids:
        id = 'layer' + row_id
        name = model[id]['Name']
        tp = model[id]['Thickness'].split(';')
        dmin, dstart, dmax = tp[:3]
        fit = 'yes' if tp[3] == 'user' else 'no'
        rows.append([id, name, dmin, dstart, dmax, fit])

    return pd.DataFrame(rows, columns=cols)


def read_nanocalc_log(fn, subset_comment=None):
    data = pd.read_csv(fn, sep=';', engine='python')
    data.rename(columns=lambda x: x.strip('#').strip().
                replace(' ', '_').lower(), inplace=True)
    data['date'] = pd.to_datetime(data['date'], dayfirst=True)
    data.rename(columns={'date': 'dtm'}, inplace=True)
    metadata = {'dtm_start': data['dtm'].min()}
    if subset_comment:
        return metadata, data[data['comment'] == subset_comment]
    else:
        return metadata, data


def read_flowplot_log(fn, cols=None):
    # check if filename is formatted correctly
    m = re.search(r'(\d{14})$', Path(fn).stem)
    except_msg = 'Filename does not end in 14 digit timestamp (YYYYMMDDHHMMSS)'
    if m:
        try:
            dtm_start = pd.to_datetime(m.groups()[0])
        except ValueError:
            raise ValueError(except_msg)
    else:
        raise ValueError(except_msg)

    # read data
    df = pd.read_csv(fn, sep=';', header=None)

    # generate output dataset(s)
    result = []
    for i, col in cols.items():
        time_idx, data_idx = i*2, i*2+1
        if 2*i >= df.shape[1]:
            raise ValueError(f'Cannot access columns with index {time_idx} and {data_idx} (too few columns in input dataset)')
        tmp = df.iloc[:, time_idx:data_idx+1].copy()
        tmp.rename(columns={time_idx: 't', data_idx: col}, inplace=True)
        tmp = tmp[~tmp['t'].isna()]
        tmp['delta_t'] = pd.to_timedelta(tmp['t'], unit='S')
        tmp['dtm'] = tmp['delta_t'] + dtm_start
        tmp.drop(columns=['delta_t', 't'], inplace=True)
        tmp = tmp[['dtm', col]]
        result.append(tmp)

    return {'dtm_start': dtm_start}, result


def read_svc_log(fn, I0=None):
    with open(fn) as fs:
        dtm_start = fs.readline().strip()
        other = fs.readline().strip()
        metadata = {'dtm_start': pd.to_datetime(dtm_start),
                    'other': other}

        rows = []
        line = fs.readline()
        while line:
            rows.append([float(x) for x in line.strip().split()])
            line = fs.readline()

        data = pd.DataFrame(rows, columns=('delta_t', 'transmission'))

        if I0 is None:
            I0 = data.loc[0, 'transmission']  # first value

        data['absorbance'] = -np.log10(data['transmission']/I0)
        data['delta_t'] = pd.to_timedelta(data['delta_t'], unit='ms')
        data['dtm'] = (data['delta_t'] -
                       data['delta_t'].min() +
                       metadata['dtm_start'])
        data.drop(columns='delta_t', inplace=True)

    return metadata, data


def read_gisaxs_log(folder, center_timestamps=True):
    file_paths = sorted(Path(folder).glob('*.edf'))
    headers = [fabio.open(fn).header for fn in file_paths]
    fnames = [file_path.name for file_path in file_paths]
    rows = [[h['Date'], float(h['ExposureTime']), fn]
            for h, fn in zip(headers, fnames)]

    data = pd.DataFrame(rows, columns=['dtm', 'ExposureTime', 'filename'])
    data['dtm'] = pd.to_datetime(data['dtm'])

    if center_timestamps:
        data['halfexposure'] = pd.to_timedelta(data['ExposureTime'], 's')/2
        data['dtm'] = data['dtm'] + data['halfexposure']
        data.drop(columns='halfexposure', inplace=True)

    metadata = data['dtm'].min()
    return metadata, data


def merge_gi_datasets(*datasets, dtm_range=None, dtm_start=None, dtm_end=None,
                      dtm_col='dtm', dt_unit='s'):
    '''
    each dataset should be a pandas dataframe or a list of pandas dataframes.

    if dtm_range is None then all data is interpolated onto the first dataset,
    otherwise dtm_range is used dtm_start and dtm_end are variables that subset
    the dtm_range.

    dtm_col describes the mandatory column in each dataset to use for merge.

    dt_unit defines the unit of time used for the dt variable. Default to
    seconds. Any other value will return a timedelta object.
    '''
    to_process = []
    for ds in datasets:
        if isinstance(ds, (list, tuple)):
            to_process += ds
        else:
            to_process += [ds]

    # create dataframe with just the wanted dtm column
    dtm_range = to_process[0][dtm_col] if dtm_range is None else dtm_range

    if dtm_start is not None:
        if isinstance(dtm_start, str):
            dtm_start = pd.to_datetime(dtm_start)
        dtm_range = dtm_range[dtm_range >= dtm_start]
    if dtm_end is not None:
        if isinstance(dtm_end, str):
            dtm_end = pd.to_datetime(dtm_end)
        dtm_range = dtm_range[dtm_range <= dtm_end]

    result = pd.DataFrame(dtm_range)
    result.sort_values(by=dtm_col, inplace=True)
    result['dt'] = result[dtm_col] - result[dtm_col].min()

    # only convert from datetime if unit is specified
    if dt_unit == 's':
        result['dt'] = result['dt'].dt.seconds

    # join all the other datasets on nearest date.
    for ds in to_process:
        result = pd.merge_asof(result,
                               ds.sort_values(by=dtm_col),
                               on=dtm_col,
                               direction='nearest')
    return result
